{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "testblock.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hihi5456/pytorch/blob/main/testblock.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxHUo04Apk4T",
        "outputId": "e3d77370-c826-4c01-dce8-83feeff20471"
      },
      "source": [
        "!pip install adabelief-pytorch\n",
        "!pip install timm"
      ],
      "id": "LxHUo04Apk4T",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting adabelief-pytorch\n",
            "  Downloading https://files.pythonhosted.org/packages/b7/5f/7d86b3688b4c9113af460205f09820fb9e6f2691e6fa63df3ec0d5de1ce5/adabelief_pytorch-0.2.1-py3-none-any.whl\n",
            "Collecting colorama>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tabulate>=0.7 in /usr/local/lib/python3.7/dist-packages (from adabelief-pytorch) (0.8.9)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from adabelief-pytorch) (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.0->adabelief-pytorch) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.0->adabelief-pytorch) (3.7.4.3)\n",
            "Installing collected packages: colorama, adabelief-pytorch\n",
            "Successfully installed adabelief-pytorch-0.2.1 colorama-0.4.4\n",
            "Collecting timm\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/89/d94f59780b5dd973154bf506d8ce598f6bfe7cc44dd445d644d6d3be8c39/timm-0.4.5-py3-none-any.whl (287kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 6.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.8.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.9.1+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4808eaf"
      },
      "source": [
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "import torch.utils.data.dataloader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset,ConcatDataset\n",
        "import adabelief_pytorch\n",
        "import numpy  as np\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, auc\n"
      ],
      "id": "c4808eaf",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c02009e"
      },
      "source": [
        "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\n",
        "from timm.models.efficientnet_blocks import round_channels, resolve_bn_args, resolve_act_layer, BN_EPS_TF_DEFAULT\n",
        "from timm.models.efficientnet_builder import EfficientNetBuilder, decode_arch_def, efficientnet_init_weights\n",
        "from timm.models.features import FeatureInfo, FeatureHooks\n",
        "from timm.models.helpers import build_model_with_cfg, default_cfg_for_features\n",
        "from timm.models.layers import create_conv2d, create_classifier, DropPath, to_2tuple, trunc_normal_\n",
        "from timm.models.registry import register_model\n",
        "import timm\n",
        "import timm.models.efficientnet"
      ],
      "id": "8c02009e",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8a0adc7",
        "outputId": "0c6f5867-ecbc-46ae-a3f2-ee1e4e913705"
      },
      "source": [
        "!nvidia-smi\n"
      ],
      "id": "a8a0adc7",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Apr  6 05:30:07 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    28W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49d7f94b"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.manual_seed(456)\n",
        "if device =='cuda':\n",
        "    torch.cuda.manual_seed_all(456)"
      ],
      "id": "49d7f94b",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fec911c4"
      },
      "source": [
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        img_size = to_2tuple(img_size)\n",
        "        patch_size = to_2tuple(patch_size)\n",
        "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape  \n",
        "        # FIXME look at relaxing size constraints\n",
        "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
        "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        x=x.reshape(len(x),self.num_patches*3,self.patch_size[0],self.patch_size[1])\n",
        "        #print(x.shape)\n",
        "        return x\n"
      ],
      "id": "fec911c4",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70a96a52"
      },
      "source": [
        "class skipblock(nn.Module):\n",
        "  def __init__(self, in_channel,out_channel,outsize):\n",
        "    super(skipblock, self).__init__()\n",
        "    \n",
        "    \n",
        "    self.conv0=nn.Conv2d(in_channel,out_channel,1)\n",
        "    self.silu=nn.SiLU()\n",
        "    \n",
        "    self.block=nn.Sequential(\n",
        "                    \n",
        "            nn.Conv2d(out_channel, 64*out_channel, 1),\n",
        "            nn.BatchNorm2d(out_channel*64),         \n",
        "            nn.SiLU(),\n",
        "            nn.Dropout2d(),\n",
        "            \n",
        "\n",
        "            nn.Conv2d(out_channel*64, out_channel*64, 3, 1, 1),\n",
        "            nn.BatchNorm2d(out_channel*64),          \n",
        "            nn.SiLU(),\n",
        "            nn.Dropout2d(),\n",
        "\n",
        "            nn.Conv2d(out_channel*64,out_channel,1),\n",
        "            nn.BatchNorm2d(out_channel),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout2d(),\n",
        "            \n",
        "        )\n",
        "    \n",
        "    self.bn=nn.BatchNorm2d(out_channel)\n",
        "    self.pool=nn.AdaptiveAvgPool2d(outsize)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    out=self.conv0(x)\n",
        "    out=self.silu(out)\n",
        "    nn.Dropout2d()\n",
        "    skip=out\n",
        "    #print(out.shape)\n",
        "    out=self.block(out)\n",
        "    #print(out.shape)\n",
        "    out=out+skip\n",
        "    #print(out.shape)\n",
        "    out=self.bn(out)\n",
        "    out=self.pool(out)\n",
        "\n",
        "    return out"
      ],
      "id": "70a96a52",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "832ce78a"
      },
      "source": [
        "#gelu=nn.GELU()\n",
        "class testblock2(nn.Module):\n",
        "    def __init__(self,ni,num_class):\n",
        "        super(testblock2, self).__init__()\n",
        "\n",
        "        self.embeding=PatchEmbed(1500,100,3,100*100*3)\n",
        "\n",
        "        \n",
        "        self.downsample=skipblock(675,64,80)\n",
        "\n",
        "  \n",
        "        self.block0=skipblock(64,32,80)\n",
        "        \n",
        "        self.block1=skipblock(32,3,50)\n",
        "        \n",
        "        self.block2=skipblock(3,1,10)\n",
        "        \n",
        "     \n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(1*10*10,num_class),\n",
        "            \n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        out=self.embeding(x)\n",
        "        out=self.downsample(out)\n",
        "        out=self.block0(out)\n",
        "        out=self.block1(out)\n",
        "        out=self.block2(out)\n",
        "\n",
        "\n",
        "        out = out.view(out.size(0),-1)\n",
        "        out=self.classifier(out)\n",
        "        \n",
        "        return out\n"
      ],
      "id": "832ce78a",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8df42c5b"
      },
      "source": [
        "model=testblock2(3,39)\n",
        "#model=torch.nn.DataParallel(model)\n",
        "model=model.to(device)"
      ],
      "id": "8df42c5b",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQr0khsJ4DNB",
        "outputId": "627c4574-fb79-44be-f3b8-7428a9857989"
      },
      "source": [
        "x=torch.rand((1,3,1500,1500)).to(device)\n",
        "model(x)\n"
      ],
      "id": "wQr0khsJ4DNB",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0662,  0.0834,  0.2586,  0.0372, -0.3026,  0.3635, -0.0498,  0.2286,\n",
              "         -0.2595,  0.0895, -0.0852, -0.2213,  0.0255,  0.1662,  0.2744,  0.0189,\n",
              "         -0.2645, -0.4103,  0.0852, -0.1068, -0.0485,  0.3315,  0.3054, -0.0933,\n",
              "         -0.1741,  0.0458,  0.1728,  0.1702,  0.0914, -0.1630, -0.1520,  0.0795,\n",
              "         -0.1396, -0.1871, -0.0887, -0.1101, -0.0070,  0.0383,  0.0004]],\n",
              "       device='cuda:0', grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuBW_dsaDWmb",
        "outputId": "2882ad04-cb26-4fe9-ae2a-4e21c617ee97"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "id": "tuBW_dsaDWmb",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "966d12b0"
      },
      "source": [
        "PATH='/content/gdrive/MyDrive/Colab Notebooks/fundusdataset/1000images/'"
      ],
      "id": "966d12b0",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f27a3c0"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [\n",
        "     transforms.RandomRotation(45),\n",
        "     transforms.RandomHorizontalFlip(),\n",
        "     transforms.RandomCrop((1500,1500)),\n",
        "     transforms.ToTensor(),\n",
        "     \n",
        "     \n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n"
      ],
      "id": "2f27a3c0",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdf409c7",
        "outputId": "76fbe60b-0e5d-4400-972b-e4ca09953357"
      },
      "source": [
        "a=torch.rand((1,3,384,384)).to(device)\n",
        "model(a)"
      ],
      "id": "fdf409c7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0125, -0.0010, -0.0864,  0.0090, -0.0817,  0.0209,  0.0551,  0.0471,\n",
              "          0.0557, -0.0259,  0.1874, -0.0632,  0.0039, -0.0286, -0.0599,  0.0168,\n",
              "          0.0617,  0.0070, -0.0242,  0.0189,  0.0119,  0.0359,  0.0332, -0.0477,\n",
              "          0.0564,  0.0958, -0.0062, -0.1499, -0.1753,  0.0749, -0.0147,  0.0444,\n",
              "          0.0997, -0.0755, -0.0031,  0.1158, -0.1942, -0.1274, -0.1188]],\n",
              "       device='cuda:0', grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17fea431"
      },
      "source": [
        "fundus_data = torchvision.datasets.ImageFolder(root=PATH,transform=transform)\n"
      ],
      "id": "17fea431",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71d07b8d"
      },
      "source": [
        "train_size=int(0.8*len(fundus_data))\n",
        "test_size=len(fundus_data)-train_size"
      ],
      "id": "71d07b8d",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ed774a7"
      },
      "source": [
        "train_set, test_set=torch.utils.data.random_split(fundus_data,[train_size,test_size])"
      ],
      "id": "9ed774a7",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "553631a5"
      },
      "source": [
        "torch.save(train_set,PATH+'trainset.csv')\n",
        "torch.save(test_set,PATH+'testset.csv')"
      ],
      "id": "553631a5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f9ca241"
      },
      "source": [
        "train_set=torch.load(PATH+'trainset.csv')\n",
        "test_set=torch.load(PATH+'testset.csv')"
      ],
      "id": "9f9ca241",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5a8caf2"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_set,drop_last=True,\n",
        "                                          batch_size=1,\n",
        "                                          shuffle=True,\n",
        "                                          num_workers=2,\n",
        "                                          )\n",
        "test_loader = torch.utils.data.DataLoader(test_set,\n",
        "                                          batch_size=1,\n",
        "                                          shuffle=False,\n",
        "                                          num_workers=4,\n",
        "                                          )"
      ],
      "id": "e5a8caf2",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5baa30c1",
        "outputId": "f47ada3c-87d1-4c49-fc5a-3fb20d35d408"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "optimizer=adabelief_pytorch.AdaBelief(model.parameters(),lr=0.001)\n",
        "#lr_sche = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n"
      ],
      "id": "5baa30c1",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mPlease check your arguments if you have upgraded adabelief-pytorch from version 0.0.5.\n",
            "\u001b[31mModifications to default arguments:\n",
            "\u001b[31m                           eps  weight_decouple    rectify\n",
            "-----------------------  -----  -----------------  ---------\n",
            "adabelief-pytorch=0.0.5  1e-08  False              False\n",
            ">=0.1.0 (Current 0.2.0)  1e-16  True               True\n",
            "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
            "----------------------------------------------------------  ----------------------------------------------\n",
            "Recommended eps = 1e-8                                      Recommended eps = 1e-16\n",
            "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
            "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
            "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
            "\u001b[0m\n",
            "Weight decoupling enabled in AdaBelief\n",
            "Rectification enabled in AdaBelief\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0efde285",
        "outputId": "88166d6d-1168-4e34-c134-27285967da7d"
      },
      "source": [
        "print(len(train_loader))"
      ],
      "id": "0efde285",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "e9feb3eb",
        "outputId": "44be424a-1b4e-456b-d895-593307f1d8d8"
      },
      "source": [
        "trainedepochs=0\n",
        "print(len(train_loader))\n",
        "\n",
        "epochs = 500\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  model.train()\n",
        "  \n",
        "  running_loss = 0.0\n",
        "\n",
        "  for i, data in enumerate(train_loader, 0):\n",
        "    # get the inputs\n",
        "    inputs, labels = data\n",
        "    inputs=inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    \n",
        "    \n",
        "       # (numarray.max()/numarray[labels[0][0].to(torch.int).tolist()-1])\n",
        "    #loss = (1/numnum[labels[0][0].to(torch.int).tolist()-1])*criterion(outputs, labels.to(torch.float)/10)\n",
        "    loss = criterion(outputs, labels)\n",
        "    \n",
        "    #print( (numnum.max()/numnum[labels[0][0].to(torch.int).tolist()-1]))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "    running_loss += loss.item()\n",
        "    if i % 100 == 99:    \n",
        "      print('[%d, %5d] loss: %.3f' %\n",
        "            (epoch + 1, i + 1, running_loss / 20))\n",
        "      running_loss = 0.0\n",
        "    #lr_sche.step()\n",
        "  \n",
        "  if epoch % 50==49:\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for data in test_loader:\n",
        "        \n",
        "        images, labels = data\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        \n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        \n",
        "        total += labels.size(0)\n",
        "        #print(predicted ,labels)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "      print('Accuracy of the network on the {} test images: {} %%'.format(len(test_set),\n",
        "                100 * correct / total))\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "  #if ( epoch==100 or epoch==150 or epoch==200 or epoch==250 or epoch==300 or epoch==350 or epoch==400 or epoch==450 or epoch==500 or epoch==550):\n",
        "      \n",
        "  if (epoch==100 or epoch==200 or epoch==300 or  epoch==400 or epoch==500 ):\n",
        "    torch.save(optimizer.state_dict, PATH+'optimizer{}.pt'.format(trainedepochs+epoch))\n",
        "    torch.save(model.state_dict,PATH+'model{}.pt'.format(trainedepochs+epoch))\n",
        "\n",
        "        \n",
        "        \n",
        "      \n",
        "      # loop over the dataset multiple times\n",
        "\n",
        "    #Check Accuracy\n",
        "    #acc = acc_check(resnet50, testloader, epoch, save=1)\n",
        "    \n",
        "    \n",
        "torch.save(model.state_dict,PATH+'model{}.pt'.format(trainedepochs+epochs))\n",
        "torch.save(optimizer.state_dict, PATH+'optimizer{}.pt'.format(trainedepochs+epochs))\n",
        "print('Finished Training')"
      ],
      "id": "e9feb3eb",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-b35b5c268732>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m#print( (numnum.max()/numnum[labels[0][0].to(torch.int).tolist()-1]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 3.35 GiB (GPU 0; 15.90 GiB total capacity; 11.41 GiB already allocated; 2.37 GiB free; 12.65 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "053160f1",
        "outputId": "ccff96cb-c5a4-4578-f1d7-6e1e7e27ab30"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "model.eval() \n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        \n",
        "        images, labels = data\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "         \n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        \n",
        "        total += labels.size(0)\n",
        "        #print(predicted ,labels)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the {} test images: {} %%'.format(len(test_set),\n",
        "    100 * correct / total))"
      ],
      "id": "053160f1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 200 test images: 34.5 %%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}