{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "testblock.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hihi5456/pytorch/blob/main/testblock.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxHUo04Apk4T",
        "outputId": "cb45e939-3b51-494f-f0a6-97f46ccb7935"
      },
      "source": [
        "!pip install adabelief-pytorch\n",
        "!pip install timm\n",
        "!pip install gpustat"
      ],
      "id": "LxHUo04Apk4T",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting adabelief-pytorch\n",
            "  Downloading https://files.pythonhosted.org/packages/b7/5f/7d86b3688b4c9113af460205f09820fb9e6f2691e6fa63df3ec0d5de1ce5/adabelief_pytorch-0.2.1-py3-none-any.whl\n",
            "Requirement already satisfied: tabulate>=0.7 in /usr/local/lib/python3.7/dist-packages (from adabelief-pytorch) (0.8.9)\n",
            "Collecting colorama>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from adabelief-pytorch) (1.8.1+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.0->adabelief-pytorch) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.0->adabelief-pytorch) (1.19.5)\n",
            "Installing collected packages: colorama, adabelief-pytorch\n",
            "Successfully installed adabelief-pytorch-0.2.1 colorama-0.4.4\n",
            "Collecting timm\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/89/d94f59780b5dd973154bf506d8ce598f6bfe7cc44dd445d644d6d3be8c39/timm-0.4.5-py3-none-any.whl (287kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 10.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.8.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.9.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.4.5\n",
            "Collecting gpustat\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/69/d8c849715171aeabd61af7da080fdc60948b5a396d2422f1f4672e43d008/gpustat-0.6.0.tar.gz (78kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 6.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.7 in /usr/local/lib/python3.7/dist-packages (from gpustat) (1.15.0)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.7/dist-packages (from gpustat) (7.352.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from gpustat) (5.4.8)\n",
            "Collecting blessings>=1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/03/74/489f85a78247609c6b4f13733cbf3ba0d864b11aa565617b645d6fdf2a4a/blessings-1.7-py3-none-any.whl\n",
            "Building wheels for collected packages: gpustat\n",
            "  Building wheel for gpustat (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpustat: filename=gpustat-0.6.0-cp37-none-any.whl size=12621 sha256=1921ac3c8bcdc6190ea394b2b51138d5f9f88d02e919d32dfc718180995200fa\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/b4/d5/fb5b7f1d040f2ff20687e3bad6867d63155dbde5a7c10f4293\n",
            "Successfully built gpustat\n",
            "Installing collected packages: blessings, gpustat\n",
            "Successfully installed blessings-1.7 gpustat-0.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4808eaf"
      },
      "source": [
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "import torch.utils.data.dataloader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset,ConcatDataset\n",
        "import adabelief_pytorch\n",
        "import numpy  as np\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, auc\n"
      ],
      "id": "c4808eaf",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c02009e"
      },
      "source": [
        "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\n",
        "from timm.models.efficientnet_blocks import round_channels, resolve_bn_args, resolve_act_layer, BN_EPS_TF_DEFAULT\n",
        "from timm.models.efficientnet_builder import EfficientNetBuilder, decode_arch_def, efficientnet_init_weights\n",
        "from timm.models.features import FeatureInfo, FeatureHooks\n",
        "from timm.models.helpers import build_model_with_cfg, default_cfg_for_features\n",
        "from timm.models.layers import create_conv2d, create_classifier, DropPath, to_2tuple, trunc_normal_\n",
        "from timm.models.registry import register_model\n",
        "import timm\n",
        "import timm.models.efficientnet"
      ],
      "id": "8c02009e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49d7f94b"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.manual_seed(456)\n",
        "if device =='cuda':\n",
        "    torch.cuda.manual_seed_all(456)"
      ],
      "id": "49d7f94b",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fec911c4"
      },
      "source": [
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        img_size = to_2tuple(img_size)\n",
        "        patch_size = to_2tuple(patch_size)\n",
        "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape  \n",
        "        # FIXME look at relaxing size constraints\n",
        "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
        "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        x=x.reshape(len(x),self.num_patches*3,self.patch_size[0],self.patch_size[1])\n",
        "        #print(x.shape)\n",
        "        return x\n"
      ],
      "id": "fec911c4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gln_neV188W5"
      },
      "source": [
        "patch1=PatchEmbed(1500,250,3,250*250*3).to(device)"
      ],
      "id": "gln_neV188W5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEhVVpT69B_1",
        "outputId": "9d683f76-be8a-4667-a979-74edc6293e6e"
      },
      "source": [
        "sum(p.numel() for p in patch1.parameters())"
      ],
      "id": "CEhVVpT69B_1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "900030000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRxY-ZGU9YvX"
      },
      "source": [
        "class PatchEmbedV2(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=1500, patch_size=250, in_chans=3, embed_dim=36*3):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=img_size//patch_size, stride=img_size//patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.proj(x)\n",
        "        \n",
        "        #print(x.shape)\n",
        "        return x\n"
      ],
      "id": "HRxY-ZGU9YvX",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAFj_hy6-UCF"
      },
      "source": [
        "patch2=PatchEmbedV2().to(device)"
      ],
      "id": "wAFj_hy6-UCF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wvRh8R8-fKn",
        "outputId": "0bf8c1c9-e7c2-4e70-e4c6-4996cd0bc54e"
      },
      "source": [
        "sum(p.numel() for p in patch2.parameters())"
      ],
      "id": "4wvRh8R8-fKn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11772"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1N2lvU70_LNs",
        "outputId": "e95bb1ab-6d9a-4dd1-9099-109b53a7752b"
      },
      "source": [
        "x=torch.rand((1,3,1500,1500)).to(device)\n",
        "patch2(x).shape"
      ],
      "id": "1N2lvU70_LNs",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 108, 250, 250])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 108, 250, 250])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70a96a52"
      },
      "source": [
        "class skipblock(nn.Module):\n",
        "  def __init__(self, in_channel,out_channel,outsize):\n",
        "    super(skipblock, self).__init__()\n",
        "    \n",
        "    \n",
        "    self.conv0=nn.Conv2d(in_channel,out_channel,1)\n",
        "    self.silu=nn.SiLU()\n",
        "    \n",
        "    self.block=nn.Sequential(\n",
        "                    \n",
        "            nn.Conv2d(out_channel, 64*out_channel, 1),\n",
        "            nn.BatchNorm2d(out_channel*64),         \n",
        "            nn.SiLU(),\n",
        "            nn.Dropout2d(),\n",
        "            \n",
        "\n",
        "            nn.Conv2d(out_channel*64, out_channel*64, 3, 1, 1),\n",
        "            nn.BatchNorm2d(out_channel*64),          \n",
        "            nn.SiLU(),\n",
        "            nn.Dropout2d(),\n",
        "\n",
        "            nn.Conv2d(out_channel*64,out_channel,1),\n",
        "            nn.BatchNorm2d(out_channel),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout2d(),\n",
        "            \n",
        "        )\n",
        "    \n",
        "    self.bn=nn.BatchNorm2d(out_channel)\n",
        "    self.pool=nn.AdaptiveAvgPool2d(outsize)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    out=self.conv0(x)\n",
        "    out=self.silu(out)\n",
        "    nn.Dropout2d()\n",
        "    skip=out\n",
        "    #print(out.shape)\n",
        "    out=self.block(out)\n",
        "    #print(out.shape)\n",
        "    out=out+skip\n",
        "    #print(out.shape)\n",
        "    out=self.bn(out)\n",
        "    out=self.pool(out)\n",
        "\n",
        "    return out"
      ],
      "id": "70a96a52",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "832ce78a"
      },
      "source": [
        "#gelu=nn.GELU()\n",
        "class testblock2(nn.Module):\n",
        "    def __init__(self,ni,num_class):\n",
        "        super(testblock2, self).__init__()\n",
        "\n",
        "        self.embeding=PatchEmbedV2(img_size=1000, patch_size=200, in_chans=3, embed_dim=25*3)   # Filter size 5*5\n",
        "\n",
        "        \n",
        "        self.downsample=skipblock(25*3,64,100)\n",
        "\n",
        "  \n",
        "        self.block0=skipblock(64,32,80)\n",
        "        \n",
        "        self.block1=skipblock(32,3,50)\n",
        "        \n",
        "        self.block2=skipblock(3,1,10)\n",
        "        \n",
        "     \n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(1*10*10,num_class),\n",
        "            \n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        out=self.embeding(x)\n",
        "        out=self.downsample(out)\n",
        "        out=self.block0(out)\n",
        "        out=self.block1(out)\n",
        "        out=self.block2(out)\n",
        "\n",
        "\n",
        "        out = out.view(out.size(0),-1)\n",
        "        out=self.classifier(out)\n",
        "        \n",
        "        return out\n"
      ],
      "id": "832ce78a",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8df42c5b"
      },
      "source": [
        "model=testblock2(3,39)\n",
        "#model=torch.nn.DataParallel(model)\n",
        "model=model.to(device)"
      ],
      "id": "8df42c5b",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQr0khsJ4DNB",
        "outputId": "687555d6-b9a9-4488-c21f-b7b7d5957df1"
      },
      "source": [
        "x=torch.rand((1,3,1000,1000)).to(device)\n",
        "model(x)\n"
      ],
      "id": "wQr0khsJ4DNB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0733,  0.0025,  0.2459, -0.3263, -0.4121,  0.0273, -0.3621, -0.2592,\n",
              "          0.0639,  0.0242, -0.0951, -0.2612, -0.0806,  0.2265,  0.3331, -0.2779,\n",
              "         -0.1313,  0.0394, -0.2683,  0.0786,  0.3275, -0.1746,  0.0750, -0.0113,\n",
              "         -0.0730, -0.4518,  0.0775, -0.3713, -0.0083, -0.3649,  0.4653,  0.3344,\n",
              "          0.0404,  0.3865,  0.1123, -0.1075,  0.1210,  0.0279,  0.0562]],\n",
              "       device='cuda:0', grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ms9Ej6T-ABxS",
        "outputId": "bf83143b-5305-4334-e116-91b3fde9bb37"
      },
      "source": [
        "sum(p.numel() for p in model.parameters())"
      ],
      "id": "Ms9Ej6T-ABxS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "189832730"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuBW_dsaDWmb",
        "outputId": "63dd354d-f96a-48a3-ae40-370373311ae4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "id": "tuBW_dsaDWmb",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "966d12b0"
      },
      "source": [
        "PATH='/content/gdrive/MyDrive/Colab Notebooks/fundusdataset/1000images/'"
      ],
      "id": "966d12b0",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f27a3c0"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [\n",
        "     #transforms.RandomRotation(10),\n",
        "     transforms.RandomHorizontalFlip(),\n",
        "     transforms.RandomCrop((1000,1000)),\n",
        "     transforms.ToTensor(),\n",
        "     \n",
        "     \n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n"
      ],
      "id": "2f27a3c0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17fea431"
      },
      "source": [
        "fundus_data = torchvision.datasets.ImageFolder(root=PATH,transform=transform,)\n"
      ],
      "id": "17fea431",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJ55dOn7T5br"
      },
      "source": [
        "\n",
        "transform = torch.nn.Sequential(\n",
        "    \n",
        "     #transforms.RandomRotation(10),\n",
        "     transforms.RandomHorizontalFlip(),\n",
        "     transforms.RandomCrop((1000,1000)),\n",
        "     #transforms.ToTensor(),\n",
        "     \n",
        "     \n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "     \n",
        "     )\n",
        "\n",
        "transform1 = transforms.Compose(\n",
        "    [\n",
        "     transforms.ToTensor()\n",
        "     ]\n",
        "     )\n",
        "\n",
        "fundus_data = torchvision.datasets.ImageFolder(root=PATH,transform=transform1)\n",
        "\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset): \n",
        "  def __init__(self,fundus_data):\n",
        "     #self.transform=transform\n",
        "    self.data= fundus_data\n",
        "\n",
        "    \n",
        "\n",
        " \n",
        "  def __len__(self): \n",
        "    return len(self.data)\n",
        "\n",
        "  \n",
        "  def __getitem__(self, idx): \n",
        "    img, label = self.data[idx]\n",
        "    \n",
        "    if img.shape[1] < 1000:\n",
        "      temp=torch.rand(3,1000,1000)\n",
        "      temp[:,:img.shape[1],:img.shape[2]]=temp[:,:img.shape[1],:img.shape[2]]+img\n",
        "      img=temp\n",
        "\n",
        "\n",
        "    img=transform(img)\n",
        "    x_0 = torch.FloatTensor(img)\n",
        "    \n",
        "    \n",
        "    y = label\n",
        "    return x_0, y\n",
        "\n",
        "fundus_data=CustomDataset(fundus_data=fundus_data)"
      ],
      "id": "fJ55dOn7T5br",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71d07b8d"
      },
      "source": [
        "train_size=int(0.8*len(fundus_data))\n",
        "test_size=len(fundus_data)-train_size"
      ],
      "id": "71d07b8d",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ed774a7"
      },
      "source": [
        "train_set, test_set=torch.utils.data.random_split(fundus_data,[train_size,test_size])"
      ],
      "id": "9ed774a7",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "553631a5"
      },
      "source": [
        "torch.save(train_set,PATH+'trainset.csv')\n",
        "torch.save(test_set,PATH+'testset.csv')"
      ],
      "id": "553631a5",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f9ca241"
      },
      "source": [
        "train_set=torch.load(PATH+'trainset.csv')\n",
        "test_set=torch.load(PATH+'testset.csv')"
      ],
      "id": "9f9ca241",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5a8caf2"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_set,drop_last=True,\n",
        "                                          batch_size=1,\n",
        "                                          shuffle=True,\n",
        "                                          num_workers=2,\n",
        "                                          )\n",
        "test_loader = torch.utils.data.DataLoader(test_set,\n",
        "                                          batch_size=1,\n",
        "                                          shuffle=False,\n",
        "                                          num_workers=4,\n",
        "                                          )"
      ],
      "id": "e5a8caf2",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5baa30c1",
        "outputId": "dea6ad88-6a66-4a52-b1d0-ca4f7ba27d33"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "optimizer=adabelief_pytorch.AdaBelief(model.parameters(),lr=0.001)\n",
        "#lr_sche = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n"
      ],
      "id": "5baa30c1",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mPlease check your arguments if you have upgraded adabelief-pytorch from version 0.0.5.\n",
            "\u001b[31mModifications to default arguments:\n",
            "\u001b[31m                           eps  weight_decouple    rectify\n",
            "-----------------------  -----  -----------------  ---------\n",
            "adabelief-pytorch=0.0.5  1e-08  False              False\n",
            ">=0.1.0 (Current 0.2.0)  1e-16  True               True\n",
            "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
            "----------------------------------------------------------  ----------------------------------------------\n",
            "Recommended eps = 1e-8                                      Recommended eps = 1e-16\n",
            "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
            "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
            "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
            "\u001b[0m\n",
            "Weight decoupling enabled in AdaBelief\n",
            "Rectification enabled in AdaBelief\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0efde285",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3edb11b1-c60f-450e-bdfe-0a9e65c85788"
      },
      "source": [
        "print(len(train_loader))"
      ],
      "id": "0efde285",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9feb3eb",
        "outputId": "950b9845-e744-4113-a4eb-cf3297683c4c"
      },
      "source": [
        "trainedepochs=0\n",
        "print(len(train_loader))\n",
        "\n",
        "epochs = 500\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  model.train()\n",
        "  \n",
        "  running_loss = 0.0\n",
        "\n",
        "  for i, data in enumerate(train_loader, 0):\n",
        "    # get the inputs\n",
        "    inputs, labels = data\n",
        "    inputs=inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    \n",
        "    \n",
        "       # (numarray.max()/numarray[labels[0][0].to(torch.int).tolist()-1])\n",
        "    #loss = (1/numnum[labels[0][0].to(torch.int).tolist()-1])*criterion(outputs, labels.to(torch.float)/10)\n",
        "    loss = criterion(outputs, labels)\n",
        "    \n",
        "    #print( (numnum.max()/numnum[labels[0][0].to(torch.int).tolist()-1]))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "    running_loss += loss.item()\n",
        "    if i % 100 == 99:    \n",
        "      print('[%d, %5d] loss: %.3f' %\n",
        "            (epoch + 1, i + 1, running_loss / 20))\n",
        "      running_loss = 0.0\n",
        "    #lr_sche.step()\n",
        "  \n",
        "  if epoch % 50==49:\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for data in test_loader:\n",
        "        \n",
        "        images, labels = data\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        \n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        \n",
        "        total += labels.size(0)\n",
        "        #print(predicted ,labels)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "      print('Accuracy of the network on the {} test images: {} %%'.format(len(test_set),\n",
        "                100 * correct / total))\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "  #if ( epoch==100 or epoch==150 or epoch==200 or epoch==250 or epoch==300 or epoch==350 or epoch==400 or epoch==450 or epoch==500 or epoch==550):\n",
        "      \n",
        "  if (epoch==100 or epoch==200 or epoch==300 or  epoch==400 or epoch==500 ):\n",
        "    torch.save(optimizer.state_dict, PATH+'optimizer{}.pt'.format(trainedepochs+epoch))\n",
        "    torch.save(model.state_dict,PATH+'model{}.pt'.format(trainedepochs+epoch))\n",
        "\n",
        "        \n",
        "        \n",
        "      \n",
        "      # loop over the dataset multiple times\n",
        "\n",
        "    #Check Accuracy\n",
        "    #acc = acc_check(resnet50, testloader, epoch, save=1)\n",
        "    \n",
        "    \n",
        "torch.save(model.state_dict,PATH+'model{}.pt'.format(trainedepochs+epochs))\n",
        "torch.save(optimizer.state_dict, PATH+'optimizer{}.pt'.format(trainedepochs+epochs))\n",
        "print('Finished Training')"
      ],
      "id": "e9feb3eb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "800\n",
            "[1,   100] loss: 18.612\n",
            "[1,   200] loss: 18.760\n",
            "[1,   300] loss: 18.506\n",
            "[1,   400] loss: 18.540\n",
            "[1,   500] loss: 18.473\n",
            "[1,   600] loss: 18.696\n",
            "[1,   700] loss: 18.463\n",
            "[1,   800] loss: 18.757\n",
            "[2,   100] loss: 18.363\n",
            "[2,   200] loss: 18.423\n",
            "[2,   300] loss: 18.568\n",
            "[2,   400] loss: 18.358\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMLcehewCsWC"
      },
      "source": [
        "torch.cuda.empty_cache() "
      ],
      "id": "hMLcehewCsWC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "053160f1",
        "outputId": "ccff96cb-c5a4-4578-f1d7-6e1e7e27ab30"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "model.eval() \n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        \n",
        "        images, labels = data\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "         \n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        \n",
        "        total += labels.size(0)\n",
        "        #print(predicted ,labels)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the {} test images: {} %%'.format(len(test_set),\n",
        "    100 * correct / total))"
      ],
      "id": "053160f1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 200 test images: 34.5 %%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}